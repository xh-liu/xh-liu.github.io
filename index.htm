<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZFB3BPBY0T"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-ZFB3BPBY0T');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Xihui Liu, HKU, UC Berkeley, BAIR, CUHK, MMLab, 刘希慧, Computer Vision" />
	<meta name="description" content="Personal page of Xihui Liu">
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<link rel="stylesheet" href="jemdoc.css" type="text/css" />
</head>

<title>Xihui Liu</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="./">Xihui Liu</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="./">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./publications/">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./group/">Group</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./awards/">Awards and Services</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./teaching/">Teaching</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px; padding-top: 25px">
				<img class="img-responsive img-rounded" src="files/XihuiLiu-photo.jpeg" alt="" style="max-width: 220px; solid black"><br>
			</div>


			<div class="col-md-5", style="padding-right: 40px; padding-top: 25px">
				<h2>Xihui Liu 刘希慧</h2>
				<p>
					<br>Assistant Professor <br>
					<a href="https://www.hku.hk/" target="_blank">University of Hong Kong</a><br><br>
					<a href="https://mmlab.ie.cuhk.edu.hk/people.html" target="_blank">HKU-MMLab</a><br>
					<a href="https://www.eee.hku.hk/" target="_blank">Department of EEE</a><br>
					<a href="https://datascience.hku.hk/" target="_blank">Institute of Data Science</a><br>
					<a href="https://www.cs.hku.hk/" target="_blank">Department of Computer Science (by courtesy)</a><br><br>
				</p>
			</div>

			<div class="col-md-4", style="padding-right: 40px; padding-top: 25px">
                <font color="black"><b>Research Interests:</b><br>
			  		<a>Computer Vision</a> <br>
					<a>Generative Models</a> <br>
					<a>Multimodal AI</a> <br>
					<a>Embodied AI</a> <br>
					<a>AI for Science</a> <br><br>
					Email: xihuiliu@eee.hku.hk <br>
					<a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ" target="_blank">Google Scholar</a> /
					<a href="https://twitter.com/XihuiLiu" target="_blank">Twitter</a> <br>
					<a href="https://www.linkedin.com/in/xihui-liu/" target="_blank">LinkedIn</a> /
					<a href="files/XihuiLiu-CV.pdf">Resume</a>
				</font>
			</div>
			
		</div>


			

		<div class="row">
			<div class="col-md-12">
			<br>
			<p>I am an Assistant Professor at the Department of Electrical and Electronic Engineering and Institute of Data Science (IDS), The University of Hong Kong. Before joining HKU, I was a postdoc Scholar at EECS Department and <a href="https://bair.berkeley.edu/" target=_blank>BAIR</a> at UC Berkeley, advised by <a href="https://people.eecs.berkeley.edu/~trevor/" target=_blank>Prof. Trevor Darrell</a>. I obtained my Ph.D. degree from <a href="http://mmlab.ie.cuhk.edu.hk/" target=_blank>Multimedia Lab (MMLab)</a>, Chinese University of Hong Kong, supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/" target=_blank>Prof. Xiaogang Wang</a> and <a href="http://www.ee.cuhk.edu.hk/~hsli/" target=_blank>Prof. Hongsheng Li</a>. I received bachelor's degree in Electronic Engineering in <a href="http://www.tsinghua.edu.cn/publish/thu2018en/index.html" target=_blank>Tsinghua University</a>. 

			<br><br>

			My research interests cover computer vision, deep learning, and artificial intelligence, with special emphasis on generative models and multimodal AI. I am also interested in their applications in embodied AI and AI for Science. I was awarded <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/" target=_blank>Adobe Research Fellowship 2020</a>, <a href="https://risingstars21-eecs.mit.edu/participants/" target=_blank>EECS Rising Stars 2021</a>, and WAIC Rising Stars Award 2022.
			
			<br><br>

			I am actively looking for self-motivated Ph.D. students, postdoctoral scholars, research assistants, and visiting students to join my group. Please drop me an email if you are interested. Eligible students can apply for <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html" target=_blank>Hong Kong PhD Fellowship Scheme (HKPFS)</a> and <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme" target=_blank>HKU Presidential PhD Scholar Programme (HKU-PS)</a>. <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/postgraduate_scholarships" target=_blank>Postgraduate scholarships (PGS)</a> are granted to other students without HKPFS and HKU-PS. Due to the large number of emails I received, I cannot reply to all of them. But I do read all emails and reply to those that I am interested in. There's no need to send duplicate emails.
			
            </p>
			</div>
		</div>
	</div><br>



  <!-- News -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
		<ul>
			<li>
                We will organize the ICML 2024 Workshop on <a href="icml-mfm-eai.github.io" target="_blank">Multimodal Foundation Models for Embodied Agents</a> and host the <a href="https://chenyi99.github.io/ego_plan_challenge/" target="_blank">EgoPlan Challenge</a>. See you in Vienna!
            </li>
			<li>
                I serve as an Area Chair for CVPR 2024 and ACM MM 2024.</a>
            </li>
            <li>
                I am awarded World Artificial Intelligence Conference (WAIC) 2022 Rising Star Award.
            </li>
			<li>
                I am selected as one of the <a href="https://risingstars21-eecs.mit.edu/" target="_blank">EECS Rising Stars 2021</a>.
            </li>
            <li>
				I am awarded <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank">2020 Adobe Research Fellowship</a>.
			</li>
		</ul>
	</div><br>




	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Highlighted Publications (<a href="./publications/">Full List of Publications</a> and <a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ">Google Scholar</a>)
		</h3>
			<hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="publications/files/T2I-CompBench++.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation</font></b><br>
				Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				TPAMI 2025<br>
				<a href="https://ieeexplore.ieee.org/document/10847875" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/T2I-CompBench-new/" target="_blank"> [Project page]</a>
				<a href="https://github.com/Karine-Huang/T2I-CompBench" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/PAR.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Parallelized Autoregressive Visual Generation</font></b><br>
				Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, <b>Xihui Liu</b>
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2412.15119" target="_blank"> [Paper]</a>
				<a href="https://epiphqny.github.io/PAR-project/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/Epiphqny/PAR" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="publications/files/T2V-CompBench.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</font></b><br>
				Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2407.14505" target="_blank"> [Paper]</a>
				<a href="https://t2v-compbench.github.io/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/KaiyueSun98/T2V-CompBench" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/spaces/Kaiyue/T2V-CompBench_Leaderboard" target="_blank"> [LeaderBoard]</a>
                </div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICLR</div><img class="img-fluid img-rounded" src="publications/files/SJD.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding</font></b><br>
				Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				ICLR 2025 <br>
				<a href="https://arxiv.org/abs/2410.01699" target="_blank"> [Paper]</a>
				<a href="https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD/" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS Spotlight</div><img class="img-fluid img-rounded" src="publications/files/GenArtist.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</font></b><br>
				Zhenyu Wang, Aoxue Li, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <b><font color="firebrick">Spotlight</font></b><br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/e7c786024ca718f2487712bfe9f51030-Paper-Conference.pdf" target="_blank"> [Paper]</a>
				<a href="https://zhenyuw16.github.io/GenArtist_page/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/zhenyuw16/GenArtist" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/LVD-2M.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</font></b><br>
				Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1df493ec1c2530c038d94d7300b5b368-Paper-Datasets_and_Benchmarks_Track.pdf" target="_blank"> [Paper]</a>
				<a href="https://silentview.github.io/LVD-2M/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/SilentView/LVD-2M" target="_blank"> [Code and Dataset]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/Beacon.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">BEACON: Benchmark for Comprehensive RNA Tasks and Language Models</font></b><br>
				Yuchen Ren, Zhiyuan Chen, Lifeng Qiao, Hongtai Jing, Yuchen Cai, Sheng Xu, Peng Ye, Xinzhu Ma, Siqi Sun, Hongliang Yan, Dong Yuan, Wanli Ouyang, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <br>
				<a href="hhttps://proceedings.neurips.cc/paper_files/paper/2024/file/a8ea503d91320fcfe12cba61c8a6d285-Paper-Datasets_and_Benchmarks_Track.pdf" target="_blank"> [Paper]</a>
				<a href="https://silentview.github.io/LVD-2M/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/terry-r123/RNABenchmark" target="_blank"> [Code and Dataset]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/ScanReason.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Empowering 3D Visual Grounding with Reasoning Capabilities</font></b><br>
				Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, <b>Xihui Liu</b>
				<br>
				ECCV 2024 <br>
				<a href="https://arxiv.org/abs/2407.01525" target="_blank"> [Paper]</a>
				<a href="https://zcmax.github.io/projects/ScanReason/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Code]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="publications/files/DreamComposer.mov" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
				Yunhan Yang*, Yukun Huang*, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, <b>Xihui Liu</b>
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2311.17061" target="_blank"> [Paper]</a>
				<a href="https://yhyang-myron.github.io/DreamComposer/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/yhyang-myron/DreamComposer" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2312.03611" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/T2I-CompBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</font></b><br>
				Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2307.06350" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/T2I-CompBench/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/Karine-Huang/T2I-CompBench" target="_blank"> [Code]</a>
				<a href="https://connecthkuhk-my.sharepoint.com/personal/huangky_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhuangky%5Fconnect%5Fhku%5Fhk%2FDocuments%2FT2I%2DCompBench&ga=1" target="_blank"> [Data]</a>
                <a href="https://huggingface.co/papers/2307.06350" target="_blank"> [Hugging Face Daily Papers]</a>
                <a href="https://karine-h.github.io/T2I-CompBench-new/" target="_blank"> [T2I-CompBench++]</a>
                </div>
			</div><hr>


	</div><br>

	

	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>

</body>

</html>
