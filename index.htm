<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZFB3BPBY0T"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-ZFB3BPBY0T');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Xihui Liu, HKU, UC Berkeley, BAIR, CUHK, MMLab, 刘希慧, Computer Vision" />
	<meta name="description" content="Personal page of Xihui Liu">
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<link rel="stylesheet" href="jemdoc.css" type="text/css" />
</head>

<title>Xihui Liu</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="./">Xihui Liu</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="./">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./publications/">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./group/">Group</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./awards/">Awards and Services</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="./teaching/">Teaching</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px; padding-top: 25px">
				<img class="img-responsive img-rounded" src="files/XihuiLiu-photo.jpeg" alt="" style="max-width: 220px; solid black"><br>
			</div>


			<div class="col-md-5", style="padding-right: 40px; padding-top: 25px">
				<h2>Xihui Liu 刘希慧</h2>
				<p>
					<br>Assistant Professor <br>
					<a href="https://www.hku.hk/" target="_blank">University of Hong Kong</a><br><br>
					<a href="https://mmlab.ie.cuhk.edu.hk/people.html" target="_blank">HKU-MMLab</a><br>
					<a href="https://www.eee.hku.hk/" target="_blank">Department of EEE</a><br>
					<a href="https://datascience.hku.hk/" target="_blank">Institute of Data Science</a><br>
					<a href="https://www.cs.hku.hk/" target="_blank">Department of Computer Science (by courtesy)</a><br><br>
				</p>
			</div>

			<div class="col-md-4", style="padding-right: 40px; padding-top: 25px">
                <font color="black"><b>Research Interests:</b><br>
			  		<a>Computer Vision</a> <br>
					<a>Generative Models</a> <br>
					<a>Multimodal AI</a> <br>
					<a>Embodied AI</a> <br>
					<a>AI for Science</a> <br><br>
					Email: xihuiliu@eee.hku.hk <br>
					<a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ" target="_blank">Google Scholar</a> /
					<a href="https://twitter.com/XihuiLiu" target="_blank">Twitter</a> <br>
					<a href="https://www.linkedin.com/in/xihui-liu/" target="_blank">LinkedIn</a> /
					<a href="files/XihuiLiu-CV.pdf">Resume</a>
				</font>
			</div>
			
		</div>


			

		<div class="row">
			<div class="col-md-12">
			<br>
			<p>I am an Assistant Professor at the Department of Electrical and Electronic Engineering and Institute of Data Science (IDS), The University of Hong Kong. Before joining HKU, I was a postdoc Scholar at EECS Department and <a href="https://bair.berkeley.edu/" target=_blank>BAIR</a> at UC Berkeley, advised by <a href="https://people.eecs.berkeley.edu/~trevor/" target=_blank>Prof. Trevor Darrell</a>. I obtained my Ph.D. degree from <a href="http://mmlab.ie.cuhk.edu.hk/" target=_blank>Multimedia Lab (MMLab)</a>, Chinese University of Hong Kong, supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/" target=_blank>Prof. Xiaogang Wang</a> and <a href="http://www.ee.cuhk.edu.hk/~hsli/" target=_blank>Prof. Hongsheng Li</a>. I received bachelor's degree in Electronic Engineering in <a href="http://www.tsinghua.edu.cn/publish/thu2018en/index.html" target=_blank>Tsinghua University</a>. 

			<br><br>

			My research interests cover computer vision, deep learning, and artificial intelligence, with special emphasis on generative models and multimodal AI. I am also interested in their applications in embodied AI and AI for Science. I was awarded <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/" target=_blank>Adobe Research Fellowship 2020</a>, <a href="https://risingstars21-eecs.mit.edu/participants/" target=_blank>EECS Rising Stars 2021</a>, and WAIC Rising Stars Award 2022.
			
			<br><br>

			I am actively looking for self-motivated Ph.D. students, postdoctoral scholars, research assistants, and visiting students to join my group. Please drop me an email if you are interested. Eligible students can apply for <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html" target=_blank>Hong Kong PhD Fellowship Scheme (HKPFS)</a> and <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme" target=_blank>HKU Presidential PhD Scholar Programme (HKU-PS)</a>. <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/postgraduate_scholarships" target=_blank>Postgraduate scholarships (PGS)</a> are granted to other students without HKPFS and HKU-PS. Due to the large number of emails I received, I cannot reply to all of them. But I do read all emails and reply to those that I am interested in. There's no need to send duplicate emails.
			
            </p>
			</div>
		</div>
	</div><br>



  <!-- News -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
		<ul>
			<li>
                We will organize the ICML 2024 Workshop on <a href="icml-mfm-eai.github.io" target="_blank">Multimodal Foundation Models for Embodied Agents</a> and host the <a href="https://chenyi99.github.io/ego_plan_challenge/" target="_blank">EgoPlan Challenge</a>. See you in Vienna!
            </li>
			<li>
                I serve as an Area Chair for CVPR 2024 and ACM MM 2024.</a>
            </li>
            <li>
                I am awarded World Artificial Intelligence Conference (WAIC) 2022 Rising Star Award.
            </li>
			<li>
                I am selected as one of the <a href="https://risingstars21-eecs.mit.edu/" target="_blank">EECS Rising Stars 2021</a>.
            </li>
            <li>
				I am awarded <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank">2020 Adobe Research Fellowship</a>.
			</li>
		</ul>
	</div><br>




	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Selected Publications
		</h3>
		<p>Full list <a href="./publications/">here</a> and <a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ">Google Scholar</a></p>

			<hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/ScanReason.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Empowering 3D Visual Grounding with Reasoning Capabilities</font></b><br>
				Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, <b>Xihui Liu</b>
				<br>
				ECCV 2024 <br>
				<a href="https://arxiv.org/abs/2407.01525" target="_blank"> [Paper]</a>
				<a href="https://zcmax.github.io/projects/ScanReason/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Code]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="publications/files/TC4D.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">TC4D: Trajectory-Conditioned Text-to-4D Generation</font></b><br>
				Sherwin Bahmani*, Xian Liu*, Yifan Wang*, Ivan Skorokhodov, Victor Rong, Ziwei Liu, <b>Xihui Liu</b>, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell
				<br>
				ECCV 2024 <br>
				<a href="https://arxiv.org/abs/2403.17920" target="_blank"> [Paper]</a>
				<a href="https://sherwinbahmani.github.io/tc4d/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/sherwinbahmani/tc4d" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2403.17920" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/PredBench.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines</font></b><br>
				ZiDong Wang, Zeyu Lu, Di Huang, Tong He, <b>Xihui Liu</b>, Wanli Ouyang, Lei Bai
				<br>
				ECCV 2024 <br>
				Coming soon!
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="publications/files/EgoPlan-Bench.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning</font></b><br>
				Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2312.06722" target="_blank"> [Paper]</a>
				<a href="https://chenyi99.github.io/ego_plan/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ChenYi99/EgoPlan" target="_blank"> [Code]</a>
				<a href="https://chenyi99.github.io/ego_plan_challenge/" target="_blank"> [Challenge]</a>
				<a href="https://drive.google.com/drive/folders/1qVtPzhHmCgdQ5JlMeAL3OZtvbHaXktTo" target="_blank"> [Data]</a>
				<a href="https://huggingface.co/spaces/ChenYi99/EgoPlan-Bench_Leaderboard" target="_blank"> [Leaderboard]</a>
                </div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="publications/files/DiM.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis</font></b><br>
				Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				arXiv 2024 <br>
				<a href="https://www.arxiv.org/abs/2405.14224" target="_blank"> [Paper]</a>
				<a href="https://github.com/tyshiwo1/DiM-DiffusionMamba" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2405.14224" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="publications/files/4Diffusion.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">4Diffusion: Multi-view Video Diffusion Model for 4D Generation</font></b><br>
				Haiyu Zhang, Xinyuan Chen, Yaohui Wang, <b>Xihui Liu</b>, Yunhong Wang, Yu Qiao
				<br>
				arXiv 2024 <br>
				<a href="https://arxiv.org/abs/2405.20674" target="_blank"> [Paper]</a>
				<a href="https://aejion.github.io/4diffusion/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/aejion/4Diffusion" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2405.20674" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="publications/files/CompAgent.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation</font></b><br>
				Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, <b>Xihui Liu</b>, Zhenguo Li
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2401.15688" target="_blank"> [Paper]</a>
				<a href="https://github.com/zhenyuw16/CompAgent_code" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2401.15688" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICML</div><img class="img-fluid img-rounded" src="publications/files/FiT.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">FiT: Flexible Vision Transformer for Diffusion Model</font></b><br>
				Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, <b>Xihui Liu</b>, Wanli Ouyang, Lei Bai
				<br>
				ICML 2024 <br>
				<a href="https://arxiv.org/abs/2402.12376" target="_blank"> [Paper]</a>
				<a href="https://github.com/whlzy/FiT" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2402.12376" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="publications/files/DreamComposer.mov" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
				Yunhan Yang*, Yukun Huang*, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, <b>Xihui Liu</b>
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2311.17061" target="_blank"> [Paper]</a>
				<a href="https://yhyang-myron.github.io/DreamComposer/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/yhyang-myron/DreamComposer" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2312.03611" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="publications/files/HumanGaussian.gif">
				</div>
				<div class="col-md-9">
				<b><font color="black">HumanGaussian: Text-driven 3d Human Generation with Gaussian Splatting</font></b><br>
				Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, <b>Xihui Liu</b>, Ziwei Liu
				<br>
				CVPR 2024 <b><font color="firebrick">Highlight</font></b><br>
				<a href="https://arxiv.org/abs/2312.03611" target="_blank"> [Paper]</a>
				<a href="https://alvinliu0.github.io/projects/HumanGaussian" target="_blank">  [Project Page]</a>
				<a href="https://github.com/alvinliu0/HumanGaussian" target="_blank"> [Code]</a>
				<a href="https://www.youtube.com/watch?v=S3djzHoqPKY" target="_blank"> [video]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/EmbodiedScan.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</font></b><br>
				Tai Wang*, Xiaohan Mao*, Chenming Zhu*, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, <b>Xihui Liu</b>, Cewu Lu, Dahua Lin, Jiangmiao Pang
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2312.16170" target="_blank"> [Paper]</a>
				<a href="https://tai-wang.github.io/embodiedscan/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/OpenRobotLab/EmbodiedScan" target="_blank"> [Code]</a>
				<a href="https://github.com/OpenRobotLab/EmbodiedScan" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="publications/files/PTv3.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Point Transformer V3: Simpler, Faster, Stronger</font></b><br>
				Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, <b>Xihui Liu</b>, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao
				<br>
				CVPR 2024 <b><font color="firebrick">Oral</font></b><br>
				<a href="https://arxiv.org/abs/2312.10035" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/PointTransformerV3" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2312.10035" target="_blank"> [Hugging Face Daily Papers]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/PPT.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</font></b><br>
				Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, <b>Xihui Liu</b>, Kaicheng Yu, Hengshuang Zhao
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2308.09718" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/Pointcept?tab=readme-ov-file#point-prompt-training-ppt" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICLR</div><img class="img-fluid img-rounded" src="publications/files/HyperHuman.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</font></b><br>
				Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, <b>Xihui Liu</b>, Ziwei Liu, Sergey Tulyakov
				<br>
				ICLR 2024<br>
				<a href="https://arxiv.org/abs/2310.08579" target="_blank"> [Paper]</a>
				<a href="https://snap-research.github.io/HyperHuman/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/snap-research/HyperHuman" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/papers/2310.08579" target="_blank"> [Hugging Face Daily Papers]</a>
				<a href="https://www.youtube.com/watch?v=eRPZW1pwxog" target="_blank"> [Short video]</a>
				<a href="https://www.youtube.com/watch?v=CxGfbwZOcyU" target="_blank"> [Long video]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/T2I-CompBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</font></b><br>
				Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2307.06350" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/T2I-CompBench/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/Karine-Huang/T2I-CompBench" target="_blank"> [Code]</a>
				<a href="https://connecthkuhk-my.sharepoint.com/personal/huangky_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhuangky%5Fconnect%5Fhku%5Fhk%2FDocuments%2FT2I%2DCompBench&ga=1" target="_blank"> [Data]</a>
                <a href="https://huggingface.co/papers/2307.06350" target="_blank"> [Hugging Face Daily Papers]</a>
                <a href="https://karine-h.github.io/T2I-CompBench-new/" target="_blank"> [T2I-CompBench++]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/OV-Parts.jpg">
            	</div>
				<div class="col-md-9">
				<b><font color="black">OV-PARTS: Towards Open-Vocabulary Part Segmentation</font></b><br>
				Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, <b>Xihui Liu</b>, Jiangmiao Pang
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/pdf/2310.05107" target="_blank"> [Paper]</a>
				<a href="https://github.com/OpenRobotLab/OV_PARTS" target="_blank"> [Code]</a>
				<a href="https://github.com/OpenRobotLab/OV_PARTS" target="_blank"> [Data]</a>
                <a href="https://ov-parts.github.io/" target="_blank"> [Challenge]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/HPBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images</font></b><br>
				Zeyu Lu*, Di Huang*, Lei Bai*, Jingjing Qu, Chengyue Wu, <b>Xihui Liu</b>, Wanli Ouyang
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2304.13023" target="_blank"> [Paper]</a>
				<a href="https://github.com/Inf-imagine/Sentry" target="_blank">  [Project Page]</a>
				<a href="https://huggingface.co/datasets/InfImagine/FakeImageDataset" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/CorresNeRF.gif">
            	</div>
				<div class="col-md-9">
				<b><font color="black">CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</font></b><br>
				Yixing Lao, Xiaogang Xu, Zhipeng Cai, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2312.06642" target="_blank"> [Paper]</a>
				<a href="https://yxlao.github.io/corres-nerf/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/yxlao/corres-nerf" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="publications/files/DDP.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">DDP: Diffusion Model for Dense Visual Prediction</font></b><br>
				Yuanfeng Ji*, Zhe Chen*, Enze Xie, Lanqing Hong, <b>Xihui Liu</b>, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo
				<br>
				ICCV 2023<br>
				<a href="https://arxiv.org/abs/2303.17559" target="_blank"> [Paper]</a>
				<a href="https://github.com/JiYuanFeng/DDP" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/DDA.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Back to the Source: Diffusion-Driven Test-Time Adaptation</font></b><br>
				Jin Gao*, Jialing Zhang*, <b>Xihui Liu</b>, Trevor Darrell, Evan Shelhamer, Dequan Wang
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2207.03442" target="_blank"> [Paper]</a>
				<a href="https://github.com/shiyegao/DDA" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/TVTS.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Learning Transferable Spatiotemporal Representations from Natural Script Knowledge</font></b><br>
				Ziyun Zeng*, Yuying Ge*, <b>Xihui Liu</b>, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2209.15280" target="_blank"> [Paper]</a>
				<a href="https://github.com/TencentARC/TVTS" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/MSC.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</font></b><br>
				Xiaoyang Wu, Xin Wen, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2303.14191" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/Pointcept?tab=readme-ov-file#masked-scene-contrast-msc" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/RIFormer.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer</font></b><br>
				Jiahao Wang, Songyang Zhang, Yong Liu, Taiqiang Wu, Yujiu Yang, <b>Xihui Liu</b>, Kai Chen, Ping Luo, Dahua Lin
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2304.05659" target="_blank"> [Paper]</a>
				<a href="https://techmonsterwang.github.io/RIFormer/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/open-mmlab/mmpretrain/tree/main/configs/riformer" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/GLeaD.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">GLeaD: Improving GANs with A Generator-Leading Task</font></b><br>
				Qingyan Bai, Ceyuan Yang, Yinghao Xu, <b>Xihui Liu</b>, Yujiu Yang, Yujun Shen
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2212.03752" target="_blank"> [Paper]</a>
				<a href="https://ezioby.github.io/glead/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/EzioBy/glead/" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/PTv2.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</font></b><br>
				Xiaoyang Wu, Yixing Lao, Li Jiang, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				NeurIPS 2022<br>
				<a href="https://arxiv.org/abs/2210.05666" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/PointTransformerV2" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/MILES.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval</font></b><br>
				Yuying Ge, Yixiao Ge, <b>Xihui Liu</b>, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
				<br>
				ECCV 2022<br>
				<a href="https://arxiv.org/abs/2204.12408" target="_blank"> [Paper]</a>
				<a href="https://github.com/TencentARC/MCQ" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="publications/files/MCQ.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Bridging Video-text Retrieval with Multiple Choice Questions</font></b><br>
				Yuying Ge, Yixiao Ge, <b>Xihui Liu</b>, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
				<br>
				CVPR 2022 <b><font color="firebrick">Oral</font></b><br>
				<a href="https://arxiv.org/abs/2201.04850" target="_blank"> [Paper]</a>
				<a href="https://geyuying.github.io/MCQ.html" target="_blank"> [Project Page]</a>
				<a href="https://github.com/TencentARC/MCQ" target="_blank"> [Code]</a>
                </div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/comp-t2i.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Benchmark for Compositional Text-to-Image Synthesis</font></b><br>
				Dong Huk Park, Samaneh Azadi, <b>Xihui Liu</b>, Trevor Darrell, Anna Rohrbach
				<br>
				NeurIPS Datasets and Benchmarks 2021<br>
				<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf" target="_blank"> [Paper]</a>
				<a href="https://github.com/Seth-Park/comp-t2i-dataset" target="_blank"> [Code]</a>
				<a href="https://github.com/Seth-Park/comp-t2i-dataset" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/Open-Edit.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions</font></b><br>
				<b>Xihui Liu</b>, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, Hongsheng Li
				<br>
				ECCV 2020<br>
				<a href="https://arxiv.org/abs/2008.01576" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/Open-Edit" target="_blank"> [Code]</a>
				<a href="https://www.youtube.com/watch?v=8E3bwvjCHYE" target="_blank"> [Video]</a>
				<a href="https://drive.google.com/file/d/1m3JKSUotm6sRImak_qjwBMtMtd037XeK/view" target="_blank"> [Slides]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="publications/files/CC-FPSE.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</font></b><br>
				<b>Xihui Liu</b>, Guojun Yin, Jing Shao, Xiaogang Wang, Hongsheng Li
				<br>
				NeurIPS 2019<br>
				<a href="https://arxiv.org/abs/1910.06809" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/CC-FPSE" target="_blank"> [Code]</a>
				<a href="https://drive.google.com/file/d/1ocpgYFmRkG_myEMzWu7uFVx7_adJUD4r/view" target="_blank"> [Slides]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="publications/files/CAMP.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</font></b><br>
				Zihao Wang*, <b>Xihui Liu*</b>, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, Jing Shao
				<br>
				ICCV 2019<br>
				<a href="https://arxiv.org/abs/1909.05506" target="_blank"> [Paper]</a>
				<a href="https://github.com/ZihaoWang-CV/CAMP_iccv19" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="publications/files/CM-Erase.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing</font></b><br>
				<b>Xihui Liu</b>, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li
				<br>
				CVPR 2019<br>
				<a href="https://arxiv.org/abs/1903.00839" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/CM-Erase-REG" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="publications/files/STD.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</font></b><br>
				<b>Xihui Liu</b>, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang
				<br>
				ECCV 2018<br>
				<a href="https://arxiv.org/abs/1803.08314" target="_blank"> [Paper]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="publications/files/HydraPlus.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</font></b><br>
				<b>Xihui Liu</b>*, Haiyu Zhao*, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie Yan, Xiaogang Wang
				<br>
				ICCV 2017<br>
				<a href="https://arxiv.org/abs/1709.09930" target="_blank"> [Paper]</a>
				<a href="https://xh-liu.github.io/HydraPlus-Net/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank"> [Code]</a>
				<a href="https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M?resourcekey=0-CdctEkdX1j2GSMSWWfrPSQ&usp=sharing" target="_blank"> [Data]</a>
                </div>
			</div><hr>

	</div><br>

	

	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>

</body>

</html>
