<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="../jumbotron.css" rel="stylesheet">
	<script src="../js/main.js"></script>
    <script src="../js/scroll.js"></script>
    <meta name="keywords" content="Xihui Liu, HKU, UC Berkeley, BAIR, CUHK, MMLab, 刘希慧, Computer Vision" />
	<meta name="description" content="Personal page of Xihui Liu">
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<link rel="stylesheet" href="jemdoc.css" type="text/css" />
</head>

<title>Xihui Liu</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="../">Xihui Liu</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="../">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="../publications/">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="../group/">Group</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="../awards/">Awards and Services</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="../teaching/">Teaching</a>
				</li>
			</ul>
		</div>
	</nav>


	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
		</h3>
		<p><a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ">Full list on Google Scholar</a></p>

			<hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="files/T2I-CompBench++.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation</font></b><br>
				Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				TPAMI 2025<br>
				<a href="https://ieeexplore.ieee.org/document/10847875" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/T2I-CompBench-new/" target="_blank"> [Project page]</a>
				<a href="https://github.com/Karine-Huang/T2I-CompBench" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/PAR.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Parallelized Autoregressive Visual Generation</font></b><br>
				Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, <b>Xihui Liu</b>
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2412.15119" target="_blank"> [Paper]</a>
				<a href="https://epiphqny.github.io/PAR-project/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/Epiphqny/PAR" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/T2V-CompBench.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</font></b><br>
				Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2407.14505" target="_blank"> [Paper]</a>
				<a href="https://t2v-compbench.github.io/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/KaiyueSun98/T2V-CompBench" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/spaces/Kaiyue/T2V-CompBench_Leaderboard" target="_blank"> [LeaderBoard]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/T2ISafety.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation</font></b><br>
				Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, <b>Xihui Liu</b>, Lu Sheng, Jing Shao
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2501.12612" target="_blank"> [Paper]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/MBQ.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">MBQ: Modality-Balanced Quantization for Large Vision-Language Models</font></b><br>
				Shiyao Li, Yingchun Hu, Xuefei Ning, <b>Xihui Liu</b>, Ke Hong, xiaotao jia, Xiuhong Li, Yaqi Yan, PEI RAN, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2412.19509" target="_blank"> [Paper]</a>
				<a href="https://github.com/thu-nics/MBQ" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/MIDI.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</font></b><br>
				Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, <b>Xihui Liu</b>, Yan-Pei Cao, Lu Sheng
				<br>
				CVPR 2025 <br>
				<a href="https://arxiv.org/abs/2412.03558" target="_blank"> [Paper]</a>
				<a href="https://huanngzh.github.io/MIDI-Page/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/huanngzh/MIDI" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/HMAR.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">HMAR: Efficient Hierarchical Masked AutoRegressive Image Generation</font></b><br>
				Hermann Kumbong, Xian Liu, Tsung-Yi Lin, Ming-Yu Liu, <b>Xihui Liu</b>, Ziwei Liu, Daniel Y Fu, Christopher Re, David W. Romero
				<br>
				CVPR 2025 <br>
				<a href="" target="_blank"> [Paper Coming Soon]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICLR</div><img class="img-fluid img-rounded" src="files/SJD.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding</font></b><br>
				Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				ICLR 2024 <br>
				<a href="https://arxiv.org/abs/2410.01699" target="_blank"> [Paper]</a>
				<a href="https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD/" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/Moto.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Moto: Latent Motion Token as the Bridging Language for Robot Manipulation</font></b><br>
				Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2412.04445" target="_blank"> [Paper]</a>
				<a href="https://chenyi99.github.io/moto/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/TencentARC/Moto" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/SAMPart3D.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">SAMPart3D: Segment Any Part in 3D Objects</font></b><br>
				Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Y. Lam, Yan-Pei Cao, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2411.07184" target="_blank"> [Paper]</a>
				<a href="https://yhyang-myron.github.io/SAMPart3D-website/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/Pointcept/SAMPart3D" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/datasets/yhyang-myron/PartObjaverse-Tiny" target="_blank"> [Dataset]</a>
                </div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/llava-3D.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D Capabilities</font></b><br>
				Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2409.18125" target="_blank"> [Paper]</a>
				<a href="https://zcmax.github.io/projects/LLaVA-3D/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ZCMax/LLaVA-3D" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/gamefactory.mov" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">GameFactory: Creating New Games with Generative Interactive Videos</font></b><br>
				Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, <b>Xihui Liu</b>
				<br>
				arXiv 2025<br>
				<a href="https://arxiv.org/abs/2501.08325" target="_blank"> [Paper]</a>
				<a href="https://vvictoryuki.github.io/gamefactory/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/KwaiVGI/GameFactory" target="_blank"> [Code]</a>
				<a href="https://huggingface.co/datasets/KwaiVGI/GameFactory-Dataset" target="_blank"> [Dataset]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/GenMAC.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration</font></b><br>
				Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu Wang, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2412.04440" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/GenMAC/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/Karine-Huang/GenMAC" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS Spotlight</div><img class="img-fluid img-rounded" src="files/GenArtist.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</font></b><br>
				Zhenyu Wang, Aoxue Li, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <b><font color="firebrick">Spotlight</font></b><br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/e7c786024ca718f2487712bfe9f51030-Paper-Conference.pdf" target="_blank"> [Paper]</a>
				<a href="https://zhenyuw16.github.io/GenArtist_page/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/zhenyuw16/GenArtist" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/LVD-2M.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</font></b><br>
				Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1df493ec1c2530c038d94d7300b5b368-Paper-Datasets_and_Benchmarks_Track.pdf" target="_blank"> [Paper]</a>
				<a href="https://silentview.github.io/LVD-2M/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/SilentView/LVD-2M" target="_blank"> [Code and Dataset]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/Beacon.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">BEACON: Benchmark for Comprehensive RNA Tasks and Language Models</font></b><br>
				Yuchen Ren, Zhiyuan Chen, Lifeng Qiao, Hongtai Jing, Yuchen Cai, Sheng Xu, Peng Ye, Xinzhu Ma, Siqi Sun, Hongliang Yan, Dong Yuan, Wanli Ouyang, <b>Xihui Liu</b>
				<br>
				NeurIPS 2024 <br>
				<a href="hhttps://proceedings.neurips.cc/paper_files/paper/2024/file/a8ea503d91320fcfe12cba61c8a6d285-Paper-Datasets_and_Benchmarks_Track.pdf" target="_blank"> [Paper]</a>
				<a href="https://silentview.github.io/LVD-2M/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/terry-r123/RNABenchmark" target="_blank"> [Code and Dataset]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/4Difussion.gif">
				</div>
				<div class="col-md-9">
				<b><font color="black">4Diffusion: Multi-view Video Diffusion Model for 4D Generation</font></b><br>
				Haiyu Zhang, Xinyuan Chen, Yaohui Wang, <b>Xihui Liu</b>, Yunhong Wang, Yu Qiao
				<br>
				NeurIPS 2024 <br>
				<a href="https://arxiv.org/abs/2405.20674" target="_blank"> [Paper]</a>
				<a href="https://aejion.github.io/4diffusion/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/aejion/4Diffusion" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/DisCo2.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation</font></b><br>
				Yunnan Wang, Ziqiang Li, Wenyao Zhang, Zequn Zhang, Baao Xie, <b>Xihui Liu</b>, Wenjun Zeng, Xin Jin
				<br>
				NeurIPS 2024 <br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/b288470688e72f58c02031304ad6339f-Paper-Conference.pdf" target="_blank"> [Paper]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/PUMA.jpg">
				</div>
				<div class="col-md-9">
				<b><font color="black">PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</font></b><br>
				Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2410.13861" target="_blank"> [Paper]</a>
				<a href="https://rongyaofang.github.io/puma/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/rongyaofang/PUMA" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/ScanReason.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Empowering 3D Visual Grounding with Reasoning Capabilities</font></b><br>
				Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, <b>Xihui Liu</b>
				<br>
				ECCV 2024 <br>
				<a href="https://arxiv.org/abs/2407.01525" target="_blank"> [Paper]</a>
				<a href="https://zcmax.github.io/projects/ScanReason/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Code]</a>
				<a href="https://github.com/ZCMax/ScanReason" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/TC4D.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">TC4D: Trajectory-Conditioned Text-to-4D Generation</font></b><br>
				Sherwin Bahmani*, Xian Liu*, Yifan Wang*, Ivan Skorokhodov, Victor Rong, Ziwei Liu, <b>Xihui Liu</b>, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell
				<br>
				ECCV 2024 <br>
				<a href="https://arxiv.org/abs/2403.17920" target="_blank"> [Paper]</a>
				<a href="https://sherwinbahmani.github.io/tc4d/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/sherwinbahmani/tc4d" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/PredBench.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines</font></b><br>
				ZiDong Wang, Zeyu Lu, Di Huang, Tong He, <b>Xihui Liu</b>, Wanli Ouyang, Lei Bai
				<br>
				ECCV 2024 <br>
				<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07567.pdf" target="_blank"> [Paper]</a>
				<a href="https://github.com/OpenEarthLab/PredBench" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/EgoPlan-Bench.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning</font></b><br>
				Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2312.06722" target="_blank"> [Paper]</a>
				<a href="https://chenyi99.github.io/ego_plan/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/ChenYi99/EgoPlan" target="_blank"> [Code]</a>
				<a href="https://chenyi99.github.io/ego_plan_challenge/" target="_blank"> [Challenge]</a>
				<a href="https://drive.google.com/drive/folders/1qVtPzhHmCgdQ5JlMeAL3OZtvbHaXktTo" target="_blank"> [Data]</a>
				<a href="https://huggingface.co/spaces/ChenYi99/EgoPlan-Bench_Leaderboard" target="_blank"> [Leaderboard]</a>
                </div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/DiM.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis</font></b><br>
				Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				arXiv 2024 <br>
				<a href="https://www.arxiv.org/abs/2405.14224" target="_blank"> [Paper]</a>
				<a href="https://github.com/tyshiwo1/DiM-DiffusionMamba" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/EMCID.mov" type="video/mp4">
            		</video>
            	</div>
				<div class="col-md-9">
				<b><font color="black">Editing Massive Concepts in Text-to-Image Diffusion Models</font></b><br>
				Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2403.13807" target="_blank"> [Paper]</a>
				<a href="https://github.com/SilentView/EMCID/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/SilentView/EMCID" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/CompAgent.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation</font></b><br>
				Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, <b>Xihui Liu</b>, Zhenguo Li
				<br>
				arXiv 2024<br>
				<a href="https://arxiv.org/abs/2401.15688" target="_blank"> [Paper]</a>
				<a href="https://github.com/zhenyuw16/CompAgent_code" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICML</div><img class="img-fluid img-rounded" src="files/FiT.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">FiT: Flexible Vision Transformer for Diffusion Model</font></b><br>
				Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, <b>Xihui Liu</b>, Wanli Ouyang, Lei Bai
				<br>
				ICML 2024 <br>
				<a href="https://arxiv.org/abs/2402.12376" target="_blank"> [Paper]</a>
				<a href="https://github.com/whlzy/FiT" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/DreamComposer.mov" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
				Yunhan Yang*, Yukun Huang*, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, <b>Xihui Liu</b>
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2311.17061" target="_blank"> [Paper]</a>
				<a href="https://yhyang-myron.github.io/DreamComposer/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/yhyang-myron/DreamComposer" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="files/HumanGaussian.gif">
				</div>
				<div class="col-md-9">
				<b><font color="black">HumanGaussian: Text-driven 3d Human Generation with Gaussian Splatting</font></b><br>
				Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, <b>Xihui Liu</b>, Ziwei Liu
				<br>
				CVPR 2024 <b><font color="firebrick">Highlight</font></b><br>
				<a href="https://arxiv.org/abs/2312.03611" target="_blank"> [Paper]</a>
				<a href="https://alvinliu0.github.io/projects/HumanGaussian" target="_blank">  [Project Page]</a>
				<a href="https://github.com/alvinliu0/HumanGaussian" target="_blank"> [Code]</a>
				<a href="https://www.youtube.com/watch?v=S3djzHoqPKY" target="_blank"> [video]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/EmbodiedScan.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</font></b><br>
				Tai Wang*, Xiaohan Mao*, Chenming Zhu*, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, <b>Xihui Liu</b>, Cewu Lu, Dahua Lin, Jiangmiao Pang
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2312.16170" target="_blank"> [Paper]</a>
				<a href="https://tai-wang.github.io/embodiedscan/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/OpenRobotLab/EmbodiedScan" target="_blank"> [Code]</a>
				<a href="https://github.com/OpenRobotLab/EmbodiedScan" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="files/PTv3.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Point Transformer V3: Simpler, Faster, Stronger</font></b><br>
				Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, <b>Xihui Liu</b>, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao
				<br>
				CVPR 2024 <b><font color="firebrick">Oral</font></b><br>
				<a href="https://arxiv.org/abs/2312.10035" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/PointTransformerV3" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/PPT.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</font></b><br>
				Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, <b>Xihui Liu</b>, Kaicheng Yu, Hengshuang Zhao
				<br>
				CVPR 2024 <br>
				<a href="https://arxiv.org/abs/2308.09718" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/Pointcept?tab=readme-ov-file#point-prompt-training-ppt" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICLR</div><img class="img-fluid img-rounded" src="files/HyperHuman.png">
				</div>
				<div class="col-md-9">
				<b><font color="black">HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</font></b><br>
				Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, <b>Xihui Liu</b>, Ziwei Liu, Sergey Tulyakov
				<br>
				ICLR 2024<br>
				<a href="https://arxiv.org/abs/2310.08579" target="_blank"> [Paper]</a>
				<a href="https://snap-research.github.io/HyperHuman/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/snap-research/HyperHuman" target="_blank"> [Code]</a>
				<a href="https://www.youtube.com/watch?v=eRPZW1pwxog" target="_blank"> [Short video]</a>
				<a href="https://www.youtube.com/watch?v=CxGfbwZOcyU" target="_blank"> [Long video]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">WACV</div><img class="img-fluid img-rounded" src="files/shape-guided-diffusion.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Shape-Guided Diffusion with Inside-Outside Attention</font></b><br>
				Dong Huk Park*, Grace Luo*, Clayton Andrew Toste, Samaneh Azadi, <b>Xihui Liu</b>, Makrine Karalashvili, Anna Rohrbach, Trevor Darrell
				<br>
				WACV 2024<br>
				<a href="https://arxiv.org/abs/2212.00210" target="_blank"> [Paper]</a>
				<a href="https://shape-guided-diffusion.github.io/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/shape-guided-diffusion/shape-guided-diffusion" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">WACV</div><img class="img-fluid img-rounded" src="files/HDAE.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation</font></b><br>
				Zeyu Lu, Chengyue Wu, Xinyuan Chen, Yaohui Wang, Yu Qiao, <b>Xihui Liu</b>
				<br>
				WACV 2024<br>
				<a href="https://arxiv.org/abs/2304.11829" target="_blank"> [Paper]</a>
				<a href="https://github.com/whlzy/Hierarchical-Diffusion-Autoencoders" target="_blank">  [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                		<source src="files/Drag-a-Video.mov" type="video/mp4">
            		</video>
            	</div>
				<div class="col-md-9">
				<b><font color="black">Drag-A-Video: Non-rigid Video Editing with Point-based Interaction</font></b><br>
				Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				arXiv 2023<br>
				<a href="https://arxiv.org/abs/2312.02936" target="_blank"> [Paper]</a>
				<a href="https://drag-a-video.github.io/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/tyshiwo1/drag-a-video" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/T2I-CompBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</font></b><br>
				Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, <b>Xihui Liu</b>
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2307.06350" target="_blank"> [Paper]</a>
				<a href="https://karine-h.github.io/T2I-CompBench/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/Karine-Huang/T2I-CompBench" target="_blank"> [Code]</a>
				<a href="https://connecthkuhk-my.sharepoint.com/personal/huangky_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhuangky%5Fconnect%5Fhku%5Fhk%2FDocuments%2FT2I%2DCompBench&ga=1" target="_blank"> [Data]</a>
                <a href="https://karine-h.github.io/T2I-CompBench-new/" target="_blank"> [T2I-CompBench++]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/OV-Parts.jpg">
            	</div>
				<div class="col-md-9">
				<b><font color="black">OV-PARTS: Towards Open-Vocabulary Part Segmentation</font></b><br>
				Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, <b>Xihui Liu</b>, Jiangmiao Pang
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/pdf/2310.05107" target="_blank"> [Paper]</a>
				<a href="https://github.com/OpenRobotLab/OV_PARTS" target="_blank"> [Code]</a>
				<a href="https://github.com/OpenRobotLab/OV_PARTS" target="_blank"> [Data]</a>
                <a href="https://ov-parts.github.io/" target="_blank"> [Challenge]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/HPBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images</font></b><br>
				Zeyu Lu*, Di Huang*, Lei Bai*, Jingjing Qu, Chengyue Wu, <b>Xihui Liu</b>, Wanli Ouyang
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2304.13023" target="_blank"> [Paper]</a>
				<a href="https://github.com/Inf-imagine/Sentry" target="_blank">  [Project Page]</a>
				<a href="https://huggingface.co/datasets/InfImagine/FakeImageDataset" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/CorresNeRF.gif">
            	</div>
				<div class="col-md-9">
				<b><font color="black">CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</font></b><br>
				Yixing Lao, Xiaogang Xu, Zhipeng Cai, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				NeurIPS 2023<br>
				<a href="https://arxiv.org/abs/2312.06642" target="_blank"> [Paper]</a>
				<a href="https://yxlao.github.io/corres-nerf/" target="_blank">  [Project Page]</a>
				<a href="https://github.com/yxlao/corres-nerf" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="files/DDP.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">DDP: Diffusion Model for Dense Visual Prediction</font></b><br>
				Yuanfeng Ji*, Zhe Chen*, Enze Xie, Lanqing Hong, <b>Xihui Liu</b>, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo
				<br>
				ICCV 2023<br>
				<a href="https://arxiv.org/abs/2303.17559" target="_blank"> [Paper]</a>
				<a href="https://github.com/JiYuanFeng/DDP" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCVW</div><img class="img-fluid img-rounded" src="files/SAM3D.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">SAM3D: Segment Anything in 3D Scenes</font></b><br>
				Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, <b>Xihui Liu</b>
				<br>
				ICCV Workshop 2023<br>
				<a href="https://arxiv.org/abs/2306.03908" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/SegmentAnything3D" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/DDA.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Back to the Source: Diffusion-Driven Test-Time Adaptation</font></b><br>
				Jin Gao*, Jialing Zhang*, <b>Xihui Liu</b>, Trevor Darrell, Evan Shelhamer, Dequan Wang
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2207.03442" target="_blank"> [Paper]</a>
				<a href="https://github.com/shiyegao/DDA" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/TVTS.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Learning Transferable Spatiotemporal Representations from Natural Script Knowledge</font></b><br>
				Ziyun Zeng*, Yuying Ge*, <b>Xihui Liu</b>, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2209.15280" target="_blank"> [Paper]</a>
				<a href="https://github.com/TencentARC/TVTS" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/MSC.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</font></b><br>
				Xiaoyang Wu, Xin Wen, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2303.14191" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/Pointcept?tab=readme-ov-file#masked-scene-contrast-msc" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/RIFormer.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer</font></b><br>
				Jiahao Wang, Songyang Zhang, Yong Liu, Taiqiang Wu, Yujiu Yang, <b>Xihui Liu</b>, Kai Chen, Ping Luo, Dahua Lin
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2304.05659" target="_blank"> [Paper]</a>
				<a href="https://techmonsterwang.github.io/RIFormer/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/open-mmlab/mmpretrain/tree/main/configs/riformer" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/GLeaD.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">GLeaD: Improving GANs with A Generator-Leading Task</font></b><br>
				Qingyan Bai, Ceyuan Yang, Yinghao Xu, <b>Xihui Liu</b>, Yujiu Yang, Yujun Shen
				<br>
				CVPR 2023<br>
				<a href="https://arxiv.org/abs/2212.03752" target="_blank"> [Paper]</a>
				<a href="https://ezioby.github.io/glead/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/EzioBy/glead/" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">WACV</div><img class="img-fluid img-rounded" src="files/SDG.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">More Control for Free! Image Synthesis with Semantic Diffusion Guidance</font></b><br>
				<b>Xihui Liu</b>, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell
				<br>
				WACV 2023<br>
				<a href="https://arxiv.org/abs/2112.05744" target="_blank"> [Paper]</a>
				<a href="https://xh-liu.github.io/sdg/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/xh-liu/SDG_code" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/TVTSv2.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale</font></b><br>
				Ziyun Zeng, Zhan Tong, <b>Xihui Liu</b>, Bin Chen, Shu-Tao Xia, Yixiao Ge
				<br>
				arXiv 2023<br>
				<a href="https://arxiv.org/abs/2305.14173" target="_blank"> [Paper]</a>
				<a href="https://github.com/TencentARC/TVTS?tab=readme-ov-file" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/PTv2.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</font></b><br>
				Xiaoyang Wu, Yixing Lao, Li Jiang, <b>Xihui Liu</b>, Hengshuang Zhao
				<br>
				NeurIPS 2022<br>
				<a href="https://arxiv.org/abs/2210.05666" target="_blank"> [Paper]</a>
				<a href="https://github.com/Pointcept/PointTransformerV2" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/MILES.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval</font></b><br>
				Yuying Ge, Yixiao Ge, <b>Xihui Liu</b>, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
				<br>
				ECCV 2022<br>
				<a href="https://arxiv.org/abs/2204.12408" target="_blank"> [Paper]</a>
				<a href="https://github.com/TencentARC/MCQ" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="files/MCQ.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Bridging Video-text Retrieval with Multiple Choice Questions</font></b><br>
				Yuying Ge, Yixiao Ge, <b>Xihui Liu</b>, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
				<br>
				CVPR 2022 <b><font color="firebrick">Oral</font></b><br>
				<a href="https://arxiv.org/abs/2201.04850" target="_blank"> [Paper]</a>
				<a href="https://geyuying.github.io/MCQ.html" target="_blank"> [Project Page]</a>
				<a href="https://github.com/TencentARC/MCQ" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/ArtBench.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">The ArtBench Dataset: Benchmarking Generative Models with Artworks</font></b><br>
				Peiyuan Liao*, Xiuyu Li*, <b>Xihui Liu</b>, Kurt Keutzer
				<br>
				arXiv 2022<br>
				<a href="https://arxiv.org/abs/2206.11404" target="_blank"> [Paper]</a>
				<a href="https://github.com/liaopeiyuan/artbench" target="_blank"> [Project Page]</a>
				<a href="https://github.com/liaopeiyuan/artbench?tab=readme-ov-file#accessing-dataset" target="_blank"> [Data]</a>
                </div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/comp-t2i.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Benchmark for Compositional Text-to-Image Synthesis</font></b><br>
				Dong Huk Park, Samaneh Azadi, <b>Xihui Liu</b>, Trevor Darrell, Anna Rohrbach
				<br>
				NeurIPS Datasets and Benchmarks 2021<br>
				<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf" target="_blank"> [Paper]</a>
				<a href="https://github.com/Seth-Park/comp-t2i-dataset" target="_blank"> [Code]</a>
				<a href="https://github.com/Seth-Park/comp-t2i-dataset" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/Open-Edit.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions</font></b><br>
				<b>Xihui Liu</b>, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, Hongsheng Li
				<br>
				ECCV 2020<br>
				<a href="https://arxiv.org/abs/2008.01576" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/Open-Edit" target="_blank"> [Code]</a>
				<a href="https://www.youtube.com/watch?v=8E3bwvjCHYE" target="_blank"> [Video]</a>
				<a href="https://drive.google.com/file/d/1m3JKSUotm6sRImak_qjwBMtMtd037XeK/view" target="_blank"> [Slides]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="files/CC-FPSE.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</font></b><br>
				<b>Xihui Liu</b>, Guojun Yin, Jing Shao, Xiaogang Wang, Hongsheng Li
				<br>
				NeurIPS 2019<br>
				<a href="https://arxiv.org/abs/1910.06809" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/CC-FPSE" target="_blank"> [Code]</a>
				<a href="https://drive.google.com/file/d/1ocpgYFmRkG_myEMzWu7uFVx7_adJUD4r/view" target="_blank"> [Slides]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="files/CAMP.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</font></b><br>
				Zihao Wang, <b>Xihui Liu</b>, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, Jing Shao
				<br>
				ICCV 2019<br>
				<a href="https://arxiv.org/abs/1909.05506" target="_blank"> [Paper]</a>
				<a href="https://github.com/ZihaoWang-CV/CAMP_iccv19" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/CM-Erase.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing</font></b><br>
				<b>Xihui Liu</b>, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li
				<br>
				CVPR 2019<br>
				<a href="https://arxiv.org/abs/1903.00839" target="_blank"> [Paper]</a>
				<a href="https://github.com/xh-liu/CM-Erase-REG" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/STD.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</font></b><br>
				<b>Xihui Liu</b>, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang
				<br>
				ECCV 2018<br>
				<a href="https://arxiv.org/abs/1803.08314" target="_blank"> [Paper]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ECCV</div><img class="img-fluid img-rounded" src="files/ReID.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Improving Deep Visual Representation for Person Re-identiﬁcation by Global and Local Image-language Association</font></b><br>
				Dapeng Chen, Hongsheng Li, <b>Xihui Liu</b>, Yantao Shen, Jing Shao, Zejian Yuan, Xiaogang Wang
				<br>
				ECCV 2018<br>
				<a href="https://arxiv.org/abs/1808.01571" target="_blank"> [Paper]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">BMVC</div><img class="img-fluid img-rounded" src="files/LGNet.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Localization Guided Learning for Pedestrian Attribute Recognition</font></b><br>
				Pengze Liu, <b>Xihui Liu</b>, Junjie Yan, Jing Shao
				<br>
				BMVC 2018<br>
				<a href="https://arxiv.org/abs/1808.09102" target="_blank"> [Paper]</a>
				<a href="https://github.com/lpzjerry/Pedestrian-Attribute-LGNet" target="_blank"> [Code]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="files/HydraPlus.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</font></b><br>
				<b>Xihui Liu</b>*, Haiyu Zhao*, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie Yan, Xiaogang Wang
				<br>
				ICCV 2017<br>
				<a href="https://arxiv.org/abs/1709.09930" target="_blank"> [Paper]</a>
				<a href="https://xh-liu.github.io/HydraPlus-Net/" target="_blank"> [Project Page]</a>
				<a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank"> [Code]</a>
				<a href="https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M?resourcekey=0-CdctEkdX1j2GSMSWWfrPSQ&usp=sharing" target="_blank"> [Data]</a>
                </div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<div class="badge">ICCV</div><img class="img-fluid img-rounded" src="files/vehicle.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-identification</font></b><br>
				Zhongdao Wang*, Luming Tang*, <b>Xihui Liu</b>, Zhuliang Yao, Shuai Yi, Jing Shao, Junjie Yan, Shengjin Wang, Hongsheng Li, Xiaogang Wang
				<br>
				ICCV 2017<br>
				<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf" target="_blank"> [Paper]</a>
				<a href="https://github.com/Zhongdao/VehicleReIDKeyPointData" target="_blank"> [Data]</a>
                </div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="badge">CVPR</div><img class="img-fluid img-rounded" src="files/VID.png">
            	</div>
				<div class="col-md-9">
				<b><font color="black">Object Detection in Videos With Tubelet Proposal Networks</font></b><br>
				Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, <b>Xihui Liu</b>, Xiaogang Wang
				<br>
				CVPR 2017<br>
				<a href="https://arxiv.org/abs/1702.06355" target="_blank"> [Paper]</a>
				<a href="https://github.com/myfavouritekk/tpn" target="_blank"> [Code]</a>
				<a href="https://www.image-net.org/challenges/LSVRC/2016/results.php#:~:text=Task%203c%3A%20Object%20detection/tracking%20from%20video%20with%20provided%20training%20data" target="_blank"> [Winner of ImageNet ILSVRC 2016 Task 3c: Object detection/tracking from video]</a>
                </div>
	</div><br>

	

	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>

</body>

</html>
